{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yileh\\AppData\\Local\\Temp\\ipykernel_10556\\403377194.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd # for data manipulation\n"
     ]
    }
   ],
   "source": [
    "# import relevant libraries\n",
    "import os # for file handling\n",
    "import glob # for file handling\n",
    "import csv # for saving dataset information as csv\n",
    "import pandas as pd # for data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Define relevant variables and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID starts from 0 and goes up by one for each pair of image and mask\n",
    "index = 1\n",
    "\n",
    "# empty list to store the following information of each dataset:\n",
    "# dataset name, file name image, file name mask, unique ID\n",
    "data_list = []\n",
    "\n",
    "# define project path\n",
    "project_path = 'c:/Users/yileh/OneDrive/msc_data_science/master_thesis'\n",
    "\n",
    "# define dataset path\n",
    "dataset_path = 'c:/Users/yileh/OneDrive/msc_data_science/master_thesis/datasets_raw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Extract information of each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 CHASEDB1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get information of the first dataset: DRIVE\n",
    "dataset_name = 'CHASEDB1'\n",
    "file_name_image = sorted(glob.glob(os.path.join(dataset_path, dataset_name, 'images', '*.jpg')))\n",
    "file_name_mask = sorted(glob.glob(os.path.join(dataset_path, dataset_name, 'masks', '*.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the information to the dataframe\n",
    "for i in range(len(file_name_image)):\n",
    "    # append relevant information to the list\n",
    "    data_list.append({'dataset_name': dataset_name, \n",
    "                      'file_name_image': os.path.basename(file_name_image[i]), \n",
    "                      'file_name_mask': os.path.basename(file_name_mask[i]), \n",
    "                      'ID': f'{index:03d}'}) # store the unique ID as string with 3 digits (e.g., 1 -> 001)\n",
    "    # increase the index by 1 (unique ID)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 DRIVE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get information of the first dataset: DRIVE\n",
    "dataset_name = 'DRIVE'\n",
    "file_name_image = sorted(glob.glob(os.path.join(dataset_path, dataset_name, 'images', '*.tif')))\n",
    "file_name_mask = sorted(glob.glob(os.path.join(dataset_path, dataset_name, 'masks', '*.gif')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the information to the dataframe\n",
    "for i in range(len(file_name_image)):\n",
    "    # append relevant information to the list\n",
    "    data_list.append({'dataset_name': dataset_name, \n",
    "                      'file_name_image': os.path.basename(file_name_image[i]), \n",
    "                      'file_name_mask': os.path.basename(file_name_mask[i]), \n",
    "                      'ID': f'{index:03d}'}) # store the unique ID with 3 digits (e.g., 1 -> 001)\n",
    "    # increase the index by 1 (unique ID)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 FIVES dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get information of the first dataset: DRIVE\n",
    "dataset_name = 'FIVES'\n",
    "file_name_image = sorted(glob.glob(os.path.join(dataset_path, dataset_name, 'images', '*.png')))\n",
    "file_name_mask = sorted(glob.glob(os.path.join(dataset_path, dataset_name, 'masks', '*.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the images and masks that have issues as identified in the script ***exploratory_data_analysis.ipynb*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file with the image and mask file names\n",
    "with open(os.path.join(project_path, 'images_to_remove.txt'), 'r') as f:\n",
    "    images_to_remove = [line.strip() for line in f.readlines()]\n",
    " \n",
    "# remove the image and mask file names from the lists\n",
    "for image_file in images_to_remove:\n",
    "    # check if image file name is in the list and remove it if it is\n",
    "    if os.path.join(dataset_path, dataset_name, 'images', image_file) in file_name_image:\n",
    "        file_name_image.remove(os.path.join(dataset_path, dataset_name, 'images', image_file))\n",
    "        file_name_mask.remove(os.path.join(dataset_path, dataset_name, 'masks', image_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images and masks after removing images and masks with issues: 668\n"
     ]
    }
   ],
   "source": [
    "# check that image and mask file names were removed\n",
    "print(f'Number of images and masks after removing images and masks with issues: {len(file_name_image)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the information to the dataframe\n",
    "for i in range(len(file_name_image)):\n",
    "    # append relevant information to the list\n",
    "    data_list.append({'dataset_name': dataset_name, \n",
    "                      'file_name_image': os.path.basename(file_name_image[i]), \n",
    "                      'file_name_mask': os.path.basename(file_name_mask[i]), \n",
    "                      'ID': f'{index:03d}'}) # store the unique ID with 3 digits (e.g., 1 -> 001)\n",
    "    # increase the index by 1 (unique ID)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4 HRF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get information of the first dataset: DRIVE\n",
    "dataset_name = 'HRF'\n",
    "file_name_image = sorted(glob.glob(os.path.join(dataset_path, dataset_name, 'images', '*.jpg')))\n",
    "file_name_mask = sorted(glob.glob(os.path.join(dataset_path, dataset_name, 'masks', '*.tif')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the information to the dataframe\n",
    "for i in range(len(file_name_image)):\n",
    "    # append relevant information to the list\n",
    "    data_list.append({'dataset_name': dataset_name, \n",
    "                      'file_name_image': os.path.basename(file_name_image[i]), \n",
    "                      'file_name_mask': os.path.basename(file_name_mask[i]), \n",
    "                      'ID': f'{index:03d}'}) # store the unique ID with 3 digits (e.g., 1 -> 001)\n",
    "    # increase the index by 1 (unique ID)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Convert to pandas dataframe and save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframe\n",
    "df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dataset_name file_name_image       file_name_mask   ID\n",
      "0     CHASEDB1   Image_01L.jpg  Image_01L_1stHO.png  001\n",
      "1     CHASEDB1   Image_01R.jpg  Image_01R_1stHO.png  002\n",
      "2     CHASEDB1   Image_02L.jpg  Image_02L_1stHO.png  003\n",
      "3     CHASEDB1   Image_02R.jpg  Image_02R_1stHO.png  004\n",
      "4     CHASEDB1   Image_03L.jpg  Image_03L_1stHO.png  005\n"
     ]
    }
   ],
   "source": [
    "# inspect the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "df.to_csv('c:/Users/yileh/OneDrive/msc_data_science/master_thesis/dataset_indexing.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
