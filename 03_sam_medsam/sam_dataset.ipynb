{"cells":[{"cell_type":"markdown","metadata":{"id":"JBbqDDoaaNaL"},"source":["<a href=\"https://colab.research.google.com/github/bnsreenu/python_for_microscopists/blob/master/331_fine_tune_SAM_mito.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"kc__W-KpMAes"},"source":["### 1 Import relevant libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mjTTZuwuHe-k"},"outputs":[],"source":["# import relevant libraries\n","from PIL import Image # read and save images\n","import numpy as np # for data manipulation\n","import torch # for PyTorch framework\n","import torchvision.transforms.functional as TF # for tensor transformations\n","from torch.utils.data import Dataset, DataLoader # for creating custom datasets and data loaders"]},{"cell_type":"markdown","metadata":{"id":"ZVOKNzJ_MAex"},"source":["### 2 Define custom SAM dataset class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQVNaYYbDKf6"},"outputs":[],"source":["class SAMDataset(Dataset):\n","  def __init__(self, images:list, masks:list, processor, transforms=None):\n","    '''\n","    Initializes SAM dataset.\n","\n","    Args:\n","        images (list): List of PIL images.\n","        masks (list): Corresponding list of masks for images.\n","        processor: Processing function that prepares input for the SAM model.\n","        transforms (callable, optional): A function that takes in a PIL image and mask and returns a transformed version (e.g., data augmentation)\n","    '''\n","    self.images = images\n","    self.masks = masks\n","    self.processor = processor\n","    self.transforms = transforms\n","\n","  def __len__(self):\n","    '''\n","    Returns the number of items in dataset.\n","    '''\n","    return len(self.images)\n","\n","  def __getitem__(self, idx):\n","    '''\n","    Retrieves an image and its corresponding mask from the dataset.\n","\n","    Args:\n","        idx: The index of the item to retrieve.\n","\n","    Returns:\n","        inputs (dict): Dictionary containing image, mask, and bounding box prompt.\n","    '''\n","    # load image and mask\n","    image = self.images[idx]\n","    mask = self.masks[idx]\n","\n","    # convert to numpy arrays\n","    image = np.array(image)\n","    mask = np.array(mask)\n","\n","    # apply defined transformations, if any\n","    if self.transforms is not None:\n","        image, mask = self.transforms(image, mask*255)\n","        mask = mask/255\n","\n","    # get indices of points within the mask\n","    y_indices, x_indices = np.where(mask > 0)\n","\n","    # generate 1000 random indices\n","    random_indices = np.random.choice(len(x_indices), size=1750, replace=False)\n","\n","    # generate 1750 input points as prompts for the model training\n","    random_points_x = x_indices[random_indices]\n","    random_points_y = y_indices[random_indices]\n","    prompt = np.column_stack((random_points_x, random_points_y))\n","\n","    # convert images and masks to PyTorch tensors\n","    image_tensor = TF.to_tensor(image)\n","    mask_tensor = TF.to_tensor(mask)\n","\n","    # prepare image and prompt for the model using SAM processor\n","    inputs = self.processor(image, input_points=[[prompt]], return_tensors=\"pt\")\n","\n","    # remove batch dimension which the processor adds by default\n","    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n","\n","    # include ground truth mask as part of the inputs\n","    inputs['ground_truth_mask'] = mask_tensor\n","\n","    return inputs"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}